% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/trans_sae_encode_decode.R
\name{sae_encode_decode}
\alias{sae_encode_decode}
\title{Stacked Autoencoder - Encode}
\usage{
sae_encode_decode(
  input_size,
  encoding_size,
  batch_size = 32,
  num_epochs = 1000,
  learning_rate = 0.001,
  k_ae = 3
)
}
\arguments{
\item{input_size}{input size}

\item{encoding_size}{encoding size}

\item{batch_size}{size for batch learning}

\item{num_epochs}{number of epochs for training}

\item{learning_rate}{learning rate}

\item{k_ae}{number of AE layers in the stack}
}
\value{
returns a \code{sae_encode_decode} object.
}
\description{
Creates an deep learning stacked autoencoder to encode a sequence of observations.
The autoencoder layers are based on DAL Toolbox Vanilla Autoencoder
It wraps the pytorch library.
}
\examples{
#See an example of using `sae_encode_decode` at this
#[link](https://github.com/cefet-rj-dal/daltoolbox/blob/main/transf/sae_enc_decode.ipynb)
}
